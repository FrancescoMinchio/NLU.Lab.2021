{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FINAL_Assignement.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNzCXJuPlLjbuZNRL57L55y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FrancescoMinchio/NLU.Lab.2021/blob/master/FINAL_Assignement.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IYNNqualolba"
      },
      "source": [
        "# FINAL_ASSIGNEMENT\n",
        "\n",
        "*   Student name: Francesco Minchio\n",
        "*   Student contact: francesco.minchio@studenti.unitn.it\n",
        "*   Student referal: 225269\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fEIcV9Gwo5UC"
      },
      "source": [
        "## INTRODUCTION\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### **Neural Network**\n",
        "\n",
        "Neural networks are a set of algorithms, modeled loosely after the human brain, that are designed to recognize patterns. They interpret sensory data through a kind of machine perception, labeling or clustering raw input. The patterns they recognize are numerical, contained in vectors, into which all real-world data, be it images, sound, text or time series, must be translated.\n",
        "\n",
        "### **LSTM**\n",
        "\n",
        "The gradient is the value used to update a NN weight. The vanishing gradient problem is when a gradient strengths as it back propagates through time of a gradient value becomes extremely small it doesn’t contribute to much learning. In recurrent NN layers that get a small gradient update doesn’t learn those are usually the earlier layers so, because of it, don’t learn, RNNs can forget what is seen and long sequences does having short term memory. LSTM is created as the solution to short-term memory; it has internal mechanisms called gates which can regulate the flow of information. These gates can learn which data in a sequence is important to keep or throw away by doing that it learns to use relevant information to make predictions almost all state-of-the-art results based on RNN. We can find these networks in speech recognition models, speech synthesis, text generation and we can even use it to generate captions for videos. When you read review your brain subconsciously only remember important keywords if your goal is trying to judge if a certain review is good or bad. We can learn to keep only relevant information to make predictions.\n",
        "\n",
        "### **LSTM for Text Generation**\n",
        "\n",
        "Activation outputs from neurons propagate in both directions, by allowing the network to learn information in a state of memory (loop) by remembering what was previously learned. This network has a state through which it makes changes to the flow of information by remembering or forgetting trends more selectively.\n",
        "\n",
        "\n",
        "\n",
        "*   Input Layer\n",
        "*   LSTM Layer\n",
        "*   Dropout Layer\n",
        "*   Output Layer\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### **Source**\n",
        "\n",
        "New York Times Dataset (articles, comments).\n",
        "\n",
        "### **Purpose**\n",
        "\n",
        "Text generation by using LSTMs. The aim is the predict next word/token.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFUxGLCOvp7K"
      },
      "source": [
        "## CODE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hK_4ey4IKg8b"
      },
      "source": [
        "As the first step, we need to import the required libraries:\n",
        "\n",
        "Keras is an API which follows best practices for reducing cognitive load, it minimizes the number of user actions required for common use cases and it provides clear and actionable error messages."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DwnVQPnE7Q6f"
      },
      "source": [
        "#keras module for building LSTM \n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.models import Sequential\n",
        "import keras.utils as ku \n",
        "\n",
        "#set seeds for reproducability\n",
        "from numpy.random import seed\n",
        "seed(1)\n",
        "from tensorflow import random\n",
        "random.set_seed(1)\n",
        "\n",
        "#standard library\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import string, os \n",
        "\n",
        "#base class of all warning category classes (subclass of Exception)\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uyVlJhjiKkQy"
      },
      "source": [
        "# Dataset load"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tMz81ce8Ko5L"
      },
      "source": [
        "curr_dir = '../input/'\n",
        "all_headlines = []\n",
        "for filename in os.listdir(curr_dir):\n",
        "    if 'Archive' in filename:\n",
        "        article_df = pd.read_csv(curr_dir + filename)\n",
        "        all_headlines.extend(list(article_df.headline.values))\n",
        "        break\n",
        "\n",
        "all_headlines = [h for h in all_headlines if h != \"Unknown\"]\n",
        "len(all_headlines)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hvao67fRKt6L"
      },
      "source": [
        "# Dataset Preparation & Text Cleaning\n",
        "\n",
        "We require a sequence input data, as given a sequence of words/tokens.\n",
        "By using the tokenization we extract tokens from a corpus, in fact Keras is used for tokenization to obtain tokens and their index in the corpus. After that, every text in the dataset is converted into a sequence of tokens.\n",
        "In this way we obtain N-gram phrases generated by imput data which are represented as output value by a table composed of two columns. In the first we have N-gram, in the second we have the sequence of tokens. In essence, each word is associated with a sequence of numbers [ , , ... ], where each integer corresponds to the index of a word in the vocabulary containing the words of the text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "913q-2AsKw0u"
      },
      "source": [
        "#removal of punctuations and lower casing all the words\n",
        "def clean_text(txt):\n",
        "    txt = \"\".join(v for v in txt if v not in string.punctuation).lower()\n",
        "    txt = txt.encode(\"utf8\").decode(\"ascii\",'ignore')\n",
        "    return txt \n",
        "\n",
        "corpus = [clean_text(x) for x in all_headlines]\n",
        "corpus[:10]\n",
        "\n",
        "#generate sequence of N-gram Tokens\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "\n",
        "def get_sequence_of_tokens(corpus):\n",
        "    ## tokenization\n",
        "    tokenizer.fit_on_texts(corpus)\n",
        "    total_words = len(tokenizer.word_index) + 1\n",
        "    \n",
        "    ## convert data to sequence of tokens \n",
        "    input_sequences = []\n",
        "    for line in corpus:\n",
        "        token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "        for i in range(1, len(token_list)):\n",
        "            n_gram_sequence = token_list[:i+1]\n",
        "            input_sequences.append(n_gram_sequence)\n",
        "    return input_sequences, total_words\n",
        "\n",
        "inp_sequences, total_words = get_sequence_of_tokens(corpus)\n",
        "inp_sequences[:10]\n",
        "\n",
        "#get sequences of words making them equal before training the model (length)\n",
        "\n",
        "def generate_padded_sequences(input_sequences):\n",
        "    max_sequence_len = max([len(x) for x in input_sequences])\n",
        "    input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
        "    \n",
        "    ##creating predictors(N-gram sequence) and labels (next word N-gram)\n",
        "    predictors, label = input_sequences[:,:-1],input_sequences[:,-1]\n",
        "    label = ku.to_categorical(label, num_classes=total_words)\n",
        "    return predictors, label, max_sequence_len\n",
        "    ##predictor = vector X \n",
        "    ##label = vector Y\n",
        "\n",
        "predictors, label, max_sequence_len = generate_padded_sequences(inp_sequences)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7h3A6RFWK3Zs"
      },
      "source": [
        "# Model Creation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RXWjF-dyK4AD"
      },
      "source": [
        "def create_model(max_sequence_len, total_words):\n",
        "    input_len = max_sequence_len - 1\n",
        "    model = Sequential()\n",
        "    \n",
        "    ##Add Input Embedding Layer\n",
        "    model.add(Embedding(total_words, 10, input_length=input_len))\n",
        "    \n",
        "    ##Add Hidden Layer 1 - LSTM Layer\n",
        "    model.add(LSTM(100))\n",
        "    model.add(Dropout(0.1))\n",
        "    \n",
        "    ##Add Output Layer\n",
        "    model.add(Dense(total_words, activation='softmax'))\n",
        "\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "    \n",
        "    return model\n",
        "\n",
        "model = create_model(max_sequence_len, total_words)\n",
        "model.summary()\n",
        "\n",
        "#model training\n",
        "\n",
        "model.fit(predictors, label, epochs=100, verbose=5)\n",
        "\n",
        "#text generation\n",
        "\n",
        "def generate_text(seed_text, next_words, model, max_sequence_len):\n",
        "    for _ in range(next_words):\n",
        "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "        token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "        predicted = model.predict_classes(token_list, verbose=0)\n",
        "        \n",
        "        output_word = \"\"\n",
        "        for word,index in tokenizer.word_index.items():\n",
        "            if index == predicted:\n",
        "                output_word = word\n",
        "                break\n",
        "        seed_text += \" \"+output_word\n",
        "    return seed_text.title()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}